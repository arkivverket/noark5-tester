#!/usr/bin/env python
"""

Import a Noark 5 extract into the API.

Test data can be found in
../noark5-validator-k/src/resources/test-uttrekk/uttrekk1/n5uttrekk

"""

from __future__ import print_function

__license__ = 'GNU General Public License v2 or later at users choice'
__author__ = 'Petter Reinholdtsen'

import sys
sys.path.append('lib')

import argparse
import json
import os
import urllib2
from functools import partial
from lxml import etree

import n5core.endpoint

strukturns = 'http://www.arkivverket.no/standarder/noark5/arkivstruktur'

tagarkiv                 = '{%s}arkiv' % strukturns
tagarkivdel              = '{%s}arkivdel' % strukturns
tagarkivskaper           = '{%s}arkivskaper' % strukturns
tagklassifikasjonssystem = '{%s}klassifikasjonssystem' % strukturns
tagklasse                = '{%s}klasse' % strukturns
tagmappe                 = '{%s}mappe' % strukturns
tagmerknad               = '{%s}merknad' % strukturns
tagregistrering          = '{%s}registrering' % strukturns
tagkorrespondansepart    = '{%s}korrespondansepart' % strukturns
tagdokumentbeskrivelse   = '{%s}dokumentbeskrivelse' % strukturns
tagdokumentobjekt        = '{%s}dokumentobjekt' % strukturns

# List of fields to ignore during import until Nikita handle them correctly
excludes = { 'klasse': ('noekkelord',),
             
             'saksmappe' : (
                 'referanseArkivdel',
                 # FIXME these need to be created before using
                 'administrativEnhet', 'saksansvarlig',
             ),
             
             'registrering': ('noekkelord',),
             'basisregistrering': ('noekkelord',),
             'journalpost': ('noekkelord',),
             'moeteregistrering': ('noekkelord',),
             
             'dokumentbeskrivelse' : ('forfatter','referanseArkivdel'),
             
             # Exclude values related to the files that are handled
             # during upload and not during dokumentobjekt creation.
             # Dropping filestoerrelse might seem like a bad idea, but
             # it is most often a free text field that is hard to
             # parse and incorrect file size would also give a
             # checksum mismatch and get rejected anyway.  Thus
             # incorrect file size would be detected during upload
             # that way.
             'dokumentobjekt': ('referanseDokumentfil', 'filstoerrelse'),
}

metadata = {
    'arkiv': (
#        'arkivstatus',
#        'dokumentmedium',
    ),
    'arkivdel': (
#        'arkivdelstatus',
#        'dokumentmedium',
    ),
    'klassifikasjonssystem': (
#        'klassifikasjonstype',
    ),
    'klasse': (
    ),
    'mappe': (
#        'mappetype',
#        'dokumentmedium',
    ),
    'saksmappe': (
        'saksstatus',
    ),
    'registrering': (
#        'dokumentmedium',
    ),
    'journalpost': (
#        'journalposttype',
#        'journalstatus',
    ),
    'dokumentbeskrivelse': (
#        'dokumenttype',
#        'dokumentstatus',
#        'dokumentmedium',
#        'tilknyttetRegistreringSom',
    ),
    'dokumentobjekt': (
#        'variantformat',
#        'format',
    ),
    'korrespondansepartenhet': (
        'korrespondanseparttype',
    ),
    'korrespondansepartperson': (
        'korrespondanseparttype',
    ),
    'korrespondansepartintern': (
        'korrespondanseparttype',
    ),
    'postadresse': (
#        'land',
#        'postnr',
#        'poststed',
    ),
}

# FIXME figure out a better way to handle formats?
formatmap = {
    'pdf':   ('fmt/14', 'application/pdf'), # actually PDF 1.0
    'pdf/a': ('fmt/95', 'application/pdf'), # actually PDF/A 1a
    'text':  ('x-fmt/111', 'text/plain'), # Plain text
}

def format2mime(format):
    if format in formatmap:
        return formatmap[format][1]
    else:
        return 'application/octet-stream'

class Importer(n5core.endpoint.Endpoint):
    def __init__(self, baseurl):
        self.nesting = 0
        self.dryrun = False
        n5core.endpoint.Endpoint.__init__(self, baseurl)
        self.stats = {}
        self.formats = {}
        self.metadatacache = {}

    def output(self, msg):
        print("%s %s" % ('=' * self.nesting, msg))

    def printstats(self):
        print("Imported entries and their count")
        for name in sorted(self.stats.keys()):
            print("%-25s %d" % (name, self.stats[name]))
        print()
        print("Imported formats and their count")
        for format in sorted(self.formats.keys()):
            print("%-25s %d" % (format, self.formats[format]))

    def metadatalookup(self, default, entity, field, kodenavn=None):
        kode = None
        if field in self.metadatacache:
            for v in self.metadatacache[field]:
                if kodenavn == v['kodenavn']:
                    kode = v['kode']
                    return {
                        'kode': kode,
                        'kodenavn': kodenavn,
                    }
        relkey = self.relbaseurl + 'metadata/%s/' % field

        # Prefer _links entries from ny-*, but fall back to searching
        # the entire API for the relevant metadata list.
        if 'links' in default and relkey in default['_links']:
            url = default['_links'][relkey]['href']
        else:
            print("error: unable to find %s in ny-%s, searching for it" % (relkey, entity))
            url = self.findRelation(relkey)
        if url:
            (c, r) = self.json_get(url)
            j = json.loads(c)
            self.metadatacache[field] = j['results']
            for v in j['results']:
                if kodenavn == v['kodenavn']:
                    kode = v['kode']
        if not kode:
            kode = 'FIXME'
        return {
            'kode': kode,
            'kodenavn': kodenavn,
        }

    # Copied from import-email
    def create_entity(self, name, rel, parent, data):
        if rel not in parent['_links'] or 'href' not in parent['_links'][rel]:
            print(parent)
            raise Exception("unable to find %s in provided relations" % rel)
        url = parent['_links'][rel]['href']
        try:
            if self.verbose:
                print("GET %s" % url)
            (gc, gres) = self.json_get(url)
            default = json.loads(gc)
            for k in default.keys():
                if not k == '_links' and k not in data:
                    data[k] = default[k]
            for field in data.keys():
                if name in metadata and field in metadata[name]:
                    value = self.metadatalookup(default, name, field, kodenavn=data[field])
                    data[field] = value
        except urllib2.HTTPError as e:
            pass
        try:
            if self.verbose:
                print("POST: %s" % data)
            (c, res) = self.json_post(url, data)
            if name not in self.stats:
                self.stats[name] = 0
            self.stats[name] += 1
        except urllib2.HTTPError as e:
            msg = e.read()
            print("Error: ", msg)
            raise
        info = json.loads(c)
        if self.verbose:
            # Validate the stuff we send came back after storing
            for f in data.keys():
                if data[f] is not None:
                    if  f not in info:
                        print("error: field %s=%s disappeared from object" %  (f, data[f]))
                    elif data[f] != info[f]:
                        print("error: field %s=%s do not match object value %s" %  (f, data[f], info[f]))
        return info

    def import_entity(self, name, rel, parentinfo, element, subs):
        self.nesting += 1
        data = {}
        for sub in element.iterchildren():
            if self.verbose:
                self.output("%s: '%s' - '%s'" % (name, sub.tag, sub.text))
            if sub.tag in subs:
                subs[sub.tag][0].append(sub)
            field = sub.tag.replace('{%s}' % strukturns, '')
            if None != sub.text:
                if name in excludes and field in excludes[name]:
                        self.output('ignoring %s field %s [%s]' % (name, field, sub.text))
                else:
                    data[field] = sub.text
        if self.verbose or self.dryrun:
            self.output("POST: %s" % data)
        if self.dryrun:
            info = None
        else:
            if self.verbose:
                print()
                print("Creating %s: %s" % (name, data))
                print()
            info = self.create_entity(name, rel, parentinfo, data)
        for tag in subs.keys():
            for sub in subs[tag][0]:
                subs[tag][1](info, sub)
        self.nesting -= 1
        return info

    def import_arkiv(self, parentinfo, element, sub=False):
        subs = {
            tagarkiv: ([], partial(self.import_arkiv, sub=True)),
            tagarkivskaper: ([], self.import_arkivskaper),
            tagarkivdel: ([], self.import_arkivdel),
        }
        if sub:
            substr = "under"
        else:
            substr = ""
        rel = self.relbaseurl + 'arkivstruktur/ny-%sarkiv/' % substr
        self.import_entity('arkiv', rel, parentinfo, element, subs)

    def import_arkivskaper(self, parentinfo, element):
        subs = {}
        rel = self.relbaseurl + 'arkivstruktur/ny-arkivskaper/'
        self.import_entity('arkivskaper', rel, parentinfo, element, subs)

    def import_arkivdel(self, parentinfo, element):
        subs = {
            tagmappe: ([], self.import_mappe),
            tagklassifikasjonssystem: ([], self.import_klassifikasjonssystem),
            tagregistrering: ([], self.import_registrering),
        }
        rel = self.relbaseurl + 'arkivstruktur/ny-arkivdel/'
        self.import_entity('arkivdel', rel, parentinfo, element, subs)

    def import_klassifikasjonssystem(self, parentinfo, element):
        subs = {
            tagklasse: ([], self.import_klasse),
        }
        rel = self.relbaseurl + 'arkivstruktur/ny-klassifikasjonssystem/'
        self.import_entity('klassifikasjonssystem', rel, parentinfo, element, subs)

    def import_klasse(self, parentinfo, element, sub=False):
        subs = {
            tagklasse: ([], partial(self.import_klasse, sub=True)),
            tagmappe: ([], self.import_mappe),
        }
        if sub:
            substr = "under"
        else:
            substr = ""
        rel = self.relbaseurl + 'arkivstruktur/ny-%sklasse/' % substr

        self.import_entity('klasse', rel, parentinfo, element, subs)

    def import_mappe(self, parentinfo, element, sub=False):
        attrtype = '{http://www.w3.org/2001/XMLSchema-instance}type'
        name = 'mappe'
        if sub:
            substr = "under"
        else:
            substr = ""
        rel = self.relbaseurl + 'arkivstruktur/ny-%smappe/' % substr
        if attrtype in element.keys():
            etype = element.get(attrtype)
            if 'moetemappe' == etype:
                name = 'moetemappe'
                rel = self.relbaseurl + 'arkivstruktur/ny-%smoetemappe/' % substr
            elif 'saksmappe' == etype:
                name = 'saksmappe'
                rel = self.relbaseurl + 'sakarkiv/ny-%ssaksmappe/' % substr
            else:
                raise ValueError("unknown %s type %s" % (element.tag, etype))
        subs = {
            tagmappe: ([], partial(self.import_mappe, sub=True) ),
            tagregistrering: ([], self.import_registrering),
#            tagmerknad: ([], self.import_merknad), # FIXME in mappe
        }
        self.import_entity(name, rel, parentinfo, element, subs)

    def import_registrering(self, parentinfo, element):
        name = 'registrering'
        rel = self.relbaseurl + 'arkivstruktur/ny-registrering/'
        attrtype = '{http://www.w3.org/2001/XMLSchema-instance}type'
        if attrtype in element.keys():
            etype = element.get(attrtype)
            if 'journalpost' == etype:
                name = 'journalpost'
                rel = self.relbaseurl + 'sakarkiv/ny-journalpost/'
            elif 'moeteregistrering' == etype:
                name = 'moeteregistrering'
                rel = self.relbaseurl + 'arkivstruktur/ny-moeteregistrering/'
            elif 'arkivnotat' == etype:
                name = 'arkivnotat'
                rel = self.relbaseurl + 'sakarkiv/ny-arkivnotat/'
            else:
                raise ValueError("unknown %s type %s" % (element.tag, etype))
        subs = {
            tagdokumentbeskrivelse: ([], self.import_dokumentbeskrivelse),
            tagkorrespondansepart: ([], self.import_korrespondansepart),
#            tagmerknad: ([], self.import_merknad), # FIXME in registrering
        }
        self.import_entity(name, rel, parentinfo, element, subs)

    def import_merknad(self, parentinfo, element):
        subs = {}
        rel = self.relbaseurl + 'arkivstruktur/ny-merknad/'
        self.import_entity('merknad', rel, parentinfo, element, subs)

    def guess_korrespondansepart_type(self, data):
        """
Guess the korrespondansepart type to use.  Fall back to person if unsure.

administrativEnhet and saksbehandler indicates korrespondansepartintern
kontaktperson indikcates korrespondansepartenhet

use navn guessing to recognize entities (' AS', ' ASA') and people?

FIXME flag inaccurate guesses somehow
"""
        print(data)
        if 'kontaktperson' not in data and 'administrativEnhet' in data:
            name = 'korrespondansepartintern'
        elif 'kontaktperson' in data \
           or -1 != data['navn'].find(' AS'):
            name = 'korrespondansepartenhet'
        else:
            name = 'korrespondansepartperson'
        print(name)
        return name


    def import_korrespondansepart(self, parentinfo, element):
        fieldmap = {
            'korrespondansepartNavn': 'navn',
            'postadresse':            'postadresse.addresselinje1|addresselinje2|addresselinje3',
            'postnummer':             'postadresse.postnr',
            'poststed':               'postadresse.poststed',
            'land':                   'postadresse.landkode',
            'epostadresse':           'kontaktinformasjon.epostadrese',
            'telefonnummer':          'kontaktinformasjon.mobiltelefon|telefon',
        }
        data = {}
        for sub in element.iterchildren():
            field = sub.tag.replace('{%s}' % strukturns, '')
            value = sub.text
            if field in fieldmap:
                if -1 != fieldmap[field].find('.'):
                    d, t = fieldmap[field].split('.')
                    if d not in data:
                        data[d] = {}
                    if -1 != t.find('|'):
                        for f in t.split('|'):
                            print("F:", f)
                            if f not in data[d]:
                                data[d][f] = value
                                break
                    else:
                        data[d][t] = value
                else:
                    data[fieldmap[field]] = value
            else:
                data[field] = value

        name = self.guess_korrespondansepart_type(data)
        rel = self.relbaseurl + 'arkivstruktur/ny-%s/' % name

        if self.verbose:
            print()
            print("Creating %s: %s" % (name, data))
            print()
        if not self.dryrun:
            info = self.create_entity(name, rel, parentinfo, data)

    def import_dokumentbeskrivelse(self, parentinfo, element):
        subs = {
            tagdokumentobjekt: ([], self.import_dokumentobjekt),
        }
        rel = self.relbaseurl + 'arkivstruktur/ny-dokumentbeskrivelse/'
        self.import_entity('dokumentbeskrivelse', rel, parentinfo, element, subs)

    def import_dokumentobjekt(self, parentinfo, element):
        rel = self.relbaseurl + 'arkivstruktur/ny-dokumentobjekt/'

        info = self.import_entity('dokumentobjekt', rel, parentinfo, element, {})

        if self.dryrun:
            return

        paths = list(element.iterchildren('{%s}referanseDokumentfil' % strukturns))
        if 1 < len(paths):
            raise ValueError('more than one referanseDokumentfil field is not allowed')
        filepath = os.path.join(self.basedir, paths[0].text)
        try:
            with open(filepath) as content:
                size = os.path.getsize(filepath)
                mimetype = format2mime( info['format'])
                if self.verbose:
                    print("Uploading %s size %d mime type %s" % (filepath,
                                                                 size, mimetype))
                uploadrel = self.relbaseurl + 'arkivstruktur/fil/'
                if uploadrel not in info['_links'] or 'href' not in info['_links'][uploadrel]:
                    print(info)
                    raise Exception("missing from created dokumentobjekt: %s" % uploadrel)
                newfilehref = info['_links'][uploadrel]['href']
                if not self.dryrun:
                    try:
                        (c, res) = self.post(str(newfilehref), content,
                                             mimetype, length=size)
                    except urllib2.HTTPError as e:
                        msg = e.read()
                        self.output('unable to POST to %s: %s' % (newfilehref, msg))
                        raise
        except IOError as e:
            self.output('unable to open %s' % filepath)
            raise
        format = info['format']
        if format not in self.formats:
            self.formats[format] = 0
        self.formats[format] += 1

    def loaddir(self, basedir):
        self.basedir = basedir
        path = os.path.join(self.basedir, "arkivstruktur.xml")
        print(path)
        parser = etree.XMLParser(remove_blank_text=True)
        fh = open(path)
        tree = etree.parse(fh, parser)
        fh.close()
        element = tree.getroot()
        if element.tag == tagarkiv:
            (c, res) = self.json_get(self.findRelation(self.relbaseurl + 'arkivstruktur/'))
            parentinfo = json.loads(c)
            self.import_arkiv(parentinfo, element)
    #    print(etree.tostring(tree))

def main():
    baseurl = "https://localhost:8092/noark5v5/"
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--baseurl", help="(default is %s)" % baseurl)
    parser.add_argument("--verbose", help="print more debug information",
                        action="store_true")
    parser.add_argument("--dryrun", help="print what would be imported, do not import",
                        action="store_true")
    parser.add_argument("basedir", nargs='+', help="directory with extract")
    args = parser.parse_args()

    if args.baseurl:
        baseurl = args.baseurl

    i = Importer(baseurl)
    i.verbose = args.verbose
    i.dryrun = args.dryrun
    i.login()

    retval = 0
    for basedir in args.basedir:
        i.loaddir(basedir)
    
    i.printstats()

    return retval

if __name__ == '__main__':
    exit(main())
